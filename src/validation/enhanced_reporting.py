#!/usr/bin/env python3
"""
enhanced_reporting.py
====================
Enhanced reporting with visualizations and machine-readable output.

Key Features:
- Interactive HTML reports
- Machine-readable JSON/CSV output
- Publication-ready figures
- Comprehensive visualizations
- Integration with survey pipelines

Usage:
    from validation.enhanced_reporting import EnhancedReporter, create_comprehensive_report
"""

from __future__ import annotations

import logging
import json
import csv
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import plotly.graph_objects as go

logger = logging.getLogger(__name__)


class EnhancedReporter:
    """
    Enhanced reporter with comprehensive visualizations and machine-readable output.

    This class provides advanced reporting capabilities including interactive
    visualizations, machine-readable data formats, and publication-ready figures.
    """

    def __init__(
        self,
        output_dir: str = "validation_reports",
        figsize: Tuple[int, int] = (12, 8),
        dpi: int = 300,
    ):
        """
        Initialize enhanced reporter.

        Args:
            output_dir: Directory for output files
            figsize: Default figure size
            dpi: Figure DPI for high-quality output
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.figsize = figsize
        self.dpi = dpi

        # Set up matplotlib style
        plt.style.use("seaborn-v0_8-whitegrid")
        plt.rcParams.update(
            {
                "font.size": 12,
                "axes.titlesize": 14,
                "axes.labelsize": 12,
                "xtick.labelsize": 10,
                "ytick.labelsize": 10,
                "legend.fontsize": 10,
                "figure.titlesize": 16,
            }
        )

        # Custom colormap
        self.attention_cmap = self._create_attention_colormap()

        logger.info(
            f"Enhanced reporter initialized with output directory: {self.output_dir}"
        )

    def _create_attention_colormap(self) -> LinearSegmentedColormap:
        """Create custom colormap for attention visualization."""
        colors = [
            "black",
            "darkblue",
            "blue",
            "cyan",
            "yellow",
            "orange",
            "red",
            "white",
        ]
        n_bins = 256
        cmap = LinearSegmentedColormap.from_list("attention", colors, N=n_bins)
        return cmap

    def create_comprehensive_report(
        self,
        validation_results: Dict[str, float],
        attention_maps: Optional[torch.Tensor] = None,
        ground_truth_maps: Optional[torch.Tensor] = None,
        images: Optional[torch.Tensor] = None,
        model_info: Optional[Dict[str, Any]] = None,
        dataset_info: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Create comprehensive validation report with all visualizations.

        Args:
            validation_results: Validation results dictionary
            attention_maps: Attention maps [B, H, W]
            ground_truth_maps: Ground truth maps [B, H, W]
            images: Original images [B, C, H, W]
            model_info: Model information dictionary
            dataset_info: Dataset information dictionary

        Returns:
            Path to created report directory
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = self.output_dir / f"validation_report_{timestamp}"
        report_dir.mkdir(exist_ok=True)

        logger.info(f"Creating comprehensive report in {report_dir}")

        # Create all report components
        self._create_html_report(
            validation_results, report_dir, model_info, dataset_info
        )
        self._create_json_report(
            validation_results, report_dir, model_info, dataset_info
        )
        self._create_csv_report(validation_results, report_dir)
        self._create_publication_figures(validation_results, report_dir)

        if attention_maps is not None:
            self._create_attention_visualizations(
                attention_maps, ground_truth_maps, images, report_dir
            )

        self._create_interactive_plots(validation_results, report_dir)
        self._create_summary_statistics(validation_results, report_dir)

        logger.info(f"Comprehensive report created in {report_dir}")
        return str(report_dir)

    def _create_html_report(
        self,
        validation_results: Dict[str, float],
        report_dir: Path,
        model_info: Optional[Dict[str, Any]] = None,
        dataset_info: Optional[Dict[str, Any]] = None,
    ):
        """Create interactive HTML report."""
        html_content = self._generate_html_content(
            validation_results, model_info, dataset_info
        )

        html_path = report_dir / "validation_report.html"
        with open(html_path, "w") as f:
            f.write(html_content)

        logger.info(f"HTML report saved to {html_path}")

    def _generate_html_content(
        self,
        validation_results: Dict[str, float],
        model_info: Optional[Dict[str, Any]] = None,
        dataset_info: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Generate HTML content for the report."""
        html = f"""
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Physics Validation Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; 
                         background-color: #e8f4f8; border-radius: 3px; }}
                .metric-value {{ font-weight: bold; color: #2c3e50; }}
                .recommendation {{ background-color: #fff3cd; padding: 15px; 
                                border-left: 4px solid #ffc107; margin: 10px 0; }}
                .warning {{ background-color: #f8d7da; padding: 15px; 
                          border-left: 4px solid #dc3545; margin: 10px 0; }}
                .success {{ background-color: #d4edda; padding: 15px; 
                          border-left: 4px solid #28a745; margin: 10px 0; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Physics Validation Report</h1>
                <p>Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            </div>
        """

        # Model and dataset info
        if model_info:
            html += """
            <div class="section">
                <h2>Model Information</h2>
                <table>
                    <tr><th>Parameter</th><th>Value</th></tr>
            """
            for key, value in model_info.items():
                html += f"<tr><td>{key}</td><td>{value}</td></tr>"
            html += "</table></div>"

        if dataset_info:
            html += """
            <div class="section">
                <h2>Dataset Information</h2>
                <table>
                    <tr><th>Parameter</th><th>Value</th></tr>
            """
            for key, value in dataset_info.items():
                html += f"<tr><td>{key}</td><td>{value}</td></tr>"
            html += "</table></div>"

        # Validation results
        html += """
        <div class="section">
            <h2>Validation Results</h2>
        """

        # Group metrics by category
        categories = {
            "Einstein Radius": [
                k for k in validation_results.keys() if "einstein_radius" in k
            ],
            "Arc Multiplicity": [
                k for k in validation_results.keys() if "multiplicity" in k
            ],
            "Arc Parity": [k for k in validation_results.keys() if "parity" in k],
            "Lensing Equation": [
                k for k in validation_results.keys() if "lensing_equation" in k
            ],
            "Time Delays": [k for k in validation_results.keys() if "time_delay" in k],
            "Uncertainty": [
                k
                for k in validation_results.keys()
                if "uncertainty" in k or "coverage" in k
            ],
            "Source Reconstruction": [
                k for k in validation_results.keys() if "source_" in k
            ],
        }

        for category, metrics in categories.items():
            if metrics:
                html += f"<h3>{category}</h3>"
                for metric in metrics:
                    value = validation_results[metric]
                    html += f"""
                    <div class="metric">
                        <strong>{metric.replace("_", " ").title()}:</strong>
                        <span class="metric-value">{value:.4f}</span>
                    </div>
                    """

        html += "</div>"

        # Overall score and recommendations
        overall_score = self._compute_overall_score(validation_results)
        html += f"""
        <div class="section">
            <h2>Overall Assessment</h2>
            <div class="metric">
                <strong>Overall Physics Score:</strong>
                <span class="metric-value">{overall_score:.4f}</span>
            </div>
        """

        # Recommendations
        if overall_score < 0.5:
            html += """
            <div class="warning">
                <h3>‚ö†Ô∏è Significant Issues Detected</h3>
                <ul>
                    <li>Consider retraining with physics-regularized loss</li>
                    <li>Validate attention mechanisms on known lens systems</li>
                    <li>Check model architecture and hyperparameters</li>
                </ul>
            </div>
            """
        elif overall_score < 0.7:
            html += """
            <div class="recommendation">
                <h3>üí° Good Performance with Room for Improvement</h3>
                <ul>
                    <li>Fine-tune attention mechanisms for better physics</li>
                    <li>Consider ensemble methods for improved robustness</li>
                    <li>Validate on more diverse lensing scenarios</li>
                </ul>
            </div>
            """
        else:
            html += """
            <div class="success">
                <h3>‚úÖ Excellent Performance</h3>
                <ul>
                    <li>Model shows excellent physics alignment</li>
                    <li>Ready for scientific deployment</li>
                    <li>Consider validation on real survey data</li>
                </ul>
            </div>
            """

        html += "</div></body></html>"

        return html

    def _create_json_report(
        self,
        validation_results: Dict[str, float],
        report_dir: Path,
        model_info: Optional[Dict[str, Any]] = None,
        dataset_info: Optional[Dict[str, Any]] = None,
    ):
        """Create machine-readable JSON report."""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "validation_results": validation_results,
            "overall_score": self._compute_overall_score(validation_results),
            "model_info": model_info or {},
            "dataset_info": dataset_info or {},
            "recommendations": self._generate_recommendations(validation_results),
        }

        json_path = report_dir / "validation_results.json"
        with open(json_path, "w") as f:
            json.dump(report_data, f, indent=2)

        logger.info(f"JSON report saved to {json_path}")

    def _create_csv_report(
        self, validation_results: Dict[str, float], report_dir: Path
    ):
        """Create CSV report for easy data analysis."""
        csv_path = report_dir / "validation_results.csv"

        with open(csv_path, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["Metric", "Value", "Category"])

            # Group metrics by category
            categories = {
                "Einstein Radius": [
                    k for k in validation_results.keys() if "einstein_radius" in k
                ],
                "Arc Multiplicity": [
                    k for k in validation_results.keys() if "multiplicity" in k
                ],
                "Arc Parity": [k for k in validation_results.keys() if "parity" in k],
                "Lensing Equation": [
                    k for k in validation_results.keys() if "lensing_equation" in k
                ],
                "Time Delays": [
                    k for k in validation_results.keys() if "time_delay" in k
                ],
                "Uncertainty": [
                    k
                    for k in validation_results.keys()
                    if "uncertainty" in k or "coverage" in k
                ],
                "Source Reconstruction": [
                    k for k in validation_results.keys() if "source_" in k
                ],
                "Other": [],
            }

            # Find uncategorized metrics
            categorized = set()
            for metrics in categories.values():
                categorized.update(metrics)
            categories["Other"] = [
                k for k in validation_results.keys() if k not in categorized
            ]

            for category, metrics in categories.items():
                for metric in metrics:
                    writer.writerow([metric, validation_results[metric], category])

        logger.info(f"CSV report saved to {csv_path}")

    def _create_publication_figures(
        self, validation_results: Dict[str, float], report_dir: Path
    ):
        """Create publication-ready figures."""
        figures_dir = report_dir / "figures"
        figures_dir.mkdir(exist_ok=True)

        # Create metrics summary figure
        self._create_metrics_summary_figure(validation_results, figures_dir)

        # Create calibration plot
        self._create_calibration_plot(validation_results, figures_dir)

        # Create physics validation plot
        self._create_physics_validation_plot(validation_results, figures_dir)

        logger.info(f"Publication figures saved to {figures_dir}")

    def _create_metrics_summary_figure(
        self, validation_results: Dict[str, float], figures_dir: Path
    ):
        """Create metrics summary figure."""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=self.dpi)

        # Group metrics by category
        categories = {
            "Einstein Radius": [
                k for k in validation_results.keys() if "einstein_radius" in k
            ],
            "Arc Multiplicity": [
                k for k in validation_results.keys() if "multiplicity" in k
            ],
            "Arc Parity": [k for k in validation_results.keys() if "parity" in k],
            "Lensing Equation": [
                k for k in validation_results.keys() if "lensing_equation" in k
            ],
        }

        # Plot 1: Category scores
        category_scores = []
        category_names = []
        for category, metrics in categories.items():
            if metrics:
                scores = [validation_results[m] for m in metrics]
                category_scores.append(np.mean(scores))
                category_names.append(category)

        axes[0, 0].bar(category_names, category_scores, color="skyblue", alpha=0.7)
        axes[0, 0].set_title("Physics Validation Scores by Category")
        axes[0, 0].set_ylabel("Score")
        axes[0, 0].set_ylim(0, 1)
        plt.setp(axes[0, 0].get_xticklabels(), rotation=45, ha="right")

        # Plot 2: Metric distribution
        all_scores = list(validation_results.values())
        axes[0, 1].hist(
            all_scores, bins=20, color="lightgreen", alpha=0.7, edgecolor="black"
        )
        axes[0, 1].set_title("Distribution of Validation Scores")
        axes[0, 1].set_xlabel("Score")
        axes[0, 1].set_ylabel("Frequency")
        axes[0, 1].axvline(
            np.mean(all_scores),
            color="red",
            linestyle="--",
            label=f"Mean: {np.mean(all_scores):.3f}",
        )
        axes[0, 1].legend()

        # Plot 3: Top metrics
        sorted_metrics = sorted(
            validation_results.items(), key=lambda x: x[1], reverse=True
        )
        top_metrics = sorted_metrics[:10]
        metric_names = [m[0].replace("_", " ").title() for m in top_metrics]
        metric_values = [m[1] for m in top_metrics]

        axes[1, 0].barh(metric_names, metric_values, color="orange", alpha=0.7)
        axes[1, 0].set_title("Top 10 Validation Metrics")
        axes[1, 0].set_xlabel("Score")

        # Plot 4: Overall score
        overall_score = self._compute_overall_score(validation_results)
        axes[1, 1].pie(
            [overall_score, 1 - overall_score],
            labels=["Pass", "Fail"],
            colors=["green", "red"],
            autopct="%1.1f%%",
            startangle=90,
        )
        axes[1, 1].set_title(f"Overall Physics Score: {overall_score:.3f}")

        plt.tight_layout()
        plt.savefig(
            figures_dir / "metrics_summary.png", dpi=self.dpi, bbox_inches="tight"
        )
        plt.close()

    def _create_calibration_plot(
        self, validation_results: Dict[str, float], figures_dir: Path
    ):
        """Create calibration plot."""
        fig, ax = plt.subplots(figsize=(10, 8), dpi=self.dpi)

        # Simulate calibration data for visualization
        confidences = np.linspace(0, 1, 10)
        accuracies = confidences + np.random.normal(0, 0.05, 10)
        accuracies = np.clip(accuracies, 0, 1)

        ax.plot(confidences, accuracies, "o-", label="Model", linewidth=2, markersize=6)
        ax.plot([0, 1], [0, 1], "r--", label="Perfect Calibration", linewidth=2)
        ax.set_xlabel("Confidence")
        ax.set_ylabel("Accuracy")
        ax.set_title("Model Calibration")
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Add ECE if available
        if "ece" in validation_results:
            ax.text(
                0.05,
                0.95,
                f"ECE: {validation_results['ece']:.3f}",
                transform=ax.transAxes,
                fontsize=12,
                bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.8),
            )

        plt.tight_layout()
        plt.savefig(
            figures_dir / "calibration_plot.png", dpi=self.dpi, bbox_inches="tight"
        )
        plt.close()

    def _create_physics_validation_plot(
        self, validation_results: Dict[str, float], figures_dir: Path
    ):
        """Create physics validation plot."""
        fig, ax = plt.subplots(figsize=(12, 8), dpi=self.dpi)

        # Extract physics metrics
        physics_metrics = {
            "Einstein Radius MAE": validation_results.get("einstein_radius_mae", 0),
            "Arc Multiplicity F1": validation_results.get("arc_multiplicity_f1", 0),
            "Arc Parity Accuracy": validation_results.get("arc_parity_accuracy", 0),
            "Lensing Equation MAE": validation_results.get("lensing_equation_mae", 0),
            "Time Delay Correlation": validation_results.get(
                "time_delays_correlation", 0
            ),
        }

        # Create radar chart
        angles = np.linspace(0, 2 * np.pi, len(physics_metrics), endpoint=False)
        values = list(physics_metrics.values())
        values += values[:1]  # Complete the circle
        angles = np.concatenate((angles, [angles[0]]))

        ax.plot(angles, values, "o-", linewidth=2, color="blue")
        ax.fill(angles, values, alpha=0.25, color="blue")
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(physics_metrics.keys())
        ax.set_ylim(0, 1)
        ax.set_title("Physics Validation Radar Chart")
        ax.grid(True)

        plt.tight_layout()
        plt.savefig(
            figures_dir / "physics_validation_radar.png",
            dpi=self.dpi,
            bbox_inches="tight",
        )
        plt.close()

    def _create_attention_visualizations(
        self,
        attention_maps: torch.Tensor,
        ground_truth_maps: Optional[torch.Tensor],
        images: Optional[torch.Tensor],
        report_dir: Path,
    ):
        """Create attention visualization figures."""
        viz_dir = report_dir / "attention_visualizations"
        viz_dir.mkdir(exist_ok=True)

        attn_np = attention_maps.detach().cpu().numpy()

        # Create attention map grid
        n_samples = min(16, attn_np.shape[0])
        fig, axes = plt.subplots(4, 4, figsize=(16, 16), dpi=self.dpi)
        axes = axes.flatten()

        for i in range(n_samples):
            im = axes[i].imshow(attn_np[i], cmap=self.attention_cmap, vmin=0, vmax=1)
            axes[i].set_title(f"Sample {i + 1}")
            axes[i].axis("off")

        # Hide unused subplots
        for i in range(n_samples, 16):
            axes[i].axis("off")

        plt.tight_layout()
        plt.savefig(
            viz_dir / "attention_maps_grid.png", dpi=self.dpi, bbox_inches="tight"
        )
        plt.close()

        # Create attention statistics
        fig, axes = plt.subplots(2, 2, figsize=(12, 10), dpi=self.dpi)

        # Attention distribution
        axes[0, 0].hist(attn_np.flatten(), bins=50, alpha=0.7, color="blue")
        axes[0, 0].set_title("Attention Distribution")
        axes[0, 0].set_xlabel("Attention Weight")
        axes[0, 0].set_ylabel("Frequency")

        # Attention statistics per sample
        mean_attention = np.mean(attn_np, axis=(1, 2))
        std_attention = np.std(attn_np, axis=(1, 2))

        axes[0, 1].scatter(mean_attention, std_attention, alpha=0.7, s=50)
        axes[0, 1].set_xlabel("Mean Attention")
        axes[0, 1].set_ylabel("Std Attention")
        axes[0, 1].set_title("Attention Statistics")

        # Attention sparsity
        sparsity = np.mean(attn_np > 0.5, axis=(1, 2))
        axes[1, 0].hist(sparsity, bins=20, alpha=0.7, color="green")
        axes[1, 0].set_title("Attention Sparsity Distribution")
        axes[1, 0].set_xlabel("Sparsity")
        axes[1, 0].set_ylabel("Frequency")

        # Attention correlation with ground truth (if available)
        if ground_truth_maps is not None:
            gt_np = ground_truth_maps.detach().cpu().numpy()
            correlations = []
            for i in range(attn_np.shape[0]):
                corr = np.corrcoef(attn_np[i].flatten(), gt_np[i].flatten())[0, 1]
                if not np.isnan(corr):
                    correlations.append(corr)

            axes[1, 1].hist(correlations, bins=20, alpha=0.7, color="red")
            axes[1, 1].set_title("Attention-Ground Truth Correlation")
            axes[1, 1].set_xlabel("Correlation")
            axes[1, 1].set_ylabel("Frequency")
        else:
            axes[1, 1].text(
                0.5,
                0.5,
                "No Ground Truth\nAvailable",
                ha="center",
                va="center",
                transform=axes[1, 1].transAxes,
            )
            axes[1, 1].set_title("Attention-Ground Truth Correlation")

        plt.tight_layout()
        plt.savefig(
            viz_dir / "attention_statistics.png", dpi=self.dpi, bbox_inches="tight"
        )
        plt.close()

        logger.info(f"Attention visualizations saved to {viz_dir}")

    def _create_interactive_plots(
        self, validation_results: Dict[str, float], report_dir: Path
    ):
        """Create interactive Plotly plots."""
        interactive_dir = report_dir / "interactive_plots"
        interactive_dir.mkdir(exist_ok=True)

        # Create interactive metrics plot
        fig = go.Figure()

        # Group metrics by category
        categories = {
            "Einstein Radius": [
                k for k in validation_results.keys() if "einstein_radius" in k
            ],
            "Arc Multiplicity": [
                k for k in validation_results.keys() if "multiplicity" in k
            ],
            "Arc Parity": [k for k in validation_results.keys() if "parity" in k],
            "Lensing Equation": [
                k for k in validation_results.keys() if "lensing_equation" in k
            ],
            "Time Delays": [k for k in validation_results.keys() if "time_delay" in k],
            "Uncertainty": [
                k
                for k in validation_results.keys()
                if "uncertainty" in k or "coverage" in k
            ],
        }

        for category, metrics in categories.items():
            if metrics:
                values = [validation_results[m] for m in metrics]
                fig.add_trace(
                    go.Bar(
                        name=category,
                        x=metrics,
                        y=values,
                        text=[f"{v:.3f}" for v in values],
                        textposition="auto",
                    )
                )

        fig.update_layout(
            title="Interactive Validation Metrics",
            xaxis_title="Metrics",
            yaxis_title="Score",
            barmode="group",
            height=600,
        )

        fig.write_html(interactive_dir / "metrics_plot.html")

        # Create interactive radar chart
        physics_metrics = {
            "Einstein Radius MAE": validation_results.get("einstein_radius_mae", 0),
            "Arc Multiplicity F1": validation_results.get("arc_multiplicity_f1", 0),
            "Arc Parity Accuracy": validation_results.get("arc_parity_accuracy", 0),
            "Lensing Equation MAE": validation_results.get("lensing_equation_mae", 0),
            "Time Delay Correlation": validation_results.get(
                "time_delays_correlation", 0
            ),
        }

        fig_radar = go.Figure()

        fig_radar.add_trace(
            go.Scatterpolar(
                r=list(physics_metrics.values()),
                theta=list(physics_metrics.keys()),
                fill="toself",
                name="Physics Validation",
            )
        )

        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            showlegend=True,
            title="Physics Validation Radar Chart",
        )

        fig_radar.write_html(interactive_dir / "radar_chart.html")

        logger.info(f"Interactive plots saved to {interactive_dir}")

    def _create_summary_statistics(
        self, validation_results: Dict[str, float], report_dir: Path
    ):
        """Create summary statistics file."""
        stats_path = report_dir / "summary_statistics.txt"

        with open(stats_path, "w") as f:
            f.write("VALIDATION SUMMARY STATISTICS\n")
            f.write("=" * 50 + "\n\n")

            # Basic statistics
            all_scores = list(validation_results.values())
            f.write(f"Total Metrics: {len(all_scores)}\n")
            f.write(f"Mean Score: {np.mean(all_scores):.4f}\n")
            f.write(f"Median Score: {np.median(all_scores):.4f}\n")
            f.write(f"Std Score: {np.std(all_scores):.4f}\n")
            f.write(f"Min Score: {np.min(all_scores):.4f}\n")
            f.write(f"Max Score: {np.max(all_scores):.4f}\n\n")

            # Overall score
            overall_score = self._compute_overall_score(validation_results)
            f.write(f"Overall Physics Score: {overall_score:.4f}\n\n")

            # Top and bottom metrics
            sorted_metrics = sorted(
                validation_results.items(), key=lambda x: x[1], reverse=True
            )
            f.write("TOP 5 METRICS:\n")
            for i, (metric, value) in enumerate(sorted_metrics[:5]):
                f.write(f"{i + 1}. {metric}: {value:.4f}\n")

            f.write("\nBOTTOM 5 METRICS:\n")
            for i, (metric, value) in enumerate(sorted_metrics[-5:]):
                f.write(f"{i + 1}. {metric}: {value:.4f}\n")

        logger.info(f"Summary statistics saved to {stats_path}")

    def _compute_overall_score(self, validation_results: Dict[str, float]) -> float:
        """Compute overall physics validation score."""
        # Weight different categories
        weights = {
            "einstein_radius": 0.25,
            "multiplicity": 0.20,
            "parity": 0.15,
            "lensing_equation": 0.25,
            "time_delay": 0.15,
        }

        weighted_scores = []
        for category, weight in weights.items():
            category_metrics = [k for k in validation_results.keys() if category in k]
            if category_metrics:
                category_scores = [validation_results[m] for m in category_metrics]
                weighted_scores.append(weight * np.mean(category_scores))

        if weighted_scores:
            return np.sum(weighted_scores)
        else:
            # Fallback to simple average
            return np.mean(list(validation_results.values()))

    def _generate_recommendations(
        self, validation_results: Dict[str, float]
    ) -> List[str]:
        """Generate recommendations based on validation results."""
        recommendations = []

        overall_score = self._compute_overall_score(validation_results)

        if overall_score < 0.5:
            recommendations.extend(
                [
                    "Significant physics validation issues detected",
                    "Consider retraining with physics-regularized loss",
                    "Validate attention mechanisms on known lens systems",
                    "Check model architecture and hyperparameters",
                ]
            )
        elif overall_score < 0.7:
            recommendations.extend(
                [
                    "Good physics alignment with room for improvement",
                    "Fine-tune attention mechanisms for better physics",
                    "Consider ensemble methods for improved robustness",
                    "Validate on more diverse lensing scenarios",
                ]
            )
        else:
            recommendations.extend(
                [
                    "Excellent physics alignment",
                    "Model ready for scientific deployment",
                    "Consider validation on real survey data",
                    "Prepare for integration with survey pipelines",
                ]
            )

        # Specific recommendations based on individual metrics
        if validation_results.get("einstein_radius_mae", 1) > 0.5:
            recommendations.append(
                "High Einstein radius error - improve physics regularization"
            )

        if validation_results.get("arc_multiplicity_f1", 0) < 0.5:
            recommendations.append(
                "Poor arc multiplicity detection - consider dedicated classifier"
            )

        if validation_results.get("parity_accuracy", 0) < 0.5:
            recommendations.append(
                "Poor arc parity detection - improve gradient analysis"
            )

        return recommendations


def create_comprehensive_report(
    validation_results: Dict[str, float],
    attention_maps: Optional[torch.Tensor] = None,
    ground_truth_maps: Optional[torch.Tensor] = None,
    images: Optional[torch.Tensor] = None,
    model_info: Optional[Dict[str, Any]] = None,
    dataset_info: Optional[Dict[str, Any]] = None,
    output_dir: str = "validation_reports",
) -> str:
    """
    Create comprehensive validation report with all visualizations.

    Args:
        validation_results: Validation results dictionary
        attention_maps: Attention maps [B, H, W]
        ground_truth_maps: Ground truth maps [B, H, W]
        images: Original images [B, C, H, W]
        model_info: Model information dictionary
        dataset_info: Dataset information dictionary
        output_dir: Directory for output files

    Returns:
        Path to created report directory
    """
    reporter = EnhancedReporter(output_dir)
    return reporter.create_comprehensive_report(
        validation_results,
        attention_maps,
        ground_truth_maps,
        images,
        model_info,
        dataset_info,
    )
