#!/usr/bin/env python3
"""
demo_p1_performance.py
=====================
Demo script showcasing P1 Performance & Scalability improvements.

Key Features Demonstrated:
- Mixed Precision Training (AMP) for 2-3x GPU speedup
- Optimized data loading with memory efficiency
- Parallel ensemble inference
- Performance benchmarking and monitoring
- Cloud deployment readiness

Usage:
    python scripts/demo_p1_performance.py --quick
    python scripts/demo_p1_performance.py --full-demo --amp
"""

# Standard library imports
import argparse
import logging
import sys
import time
from pathlib import Path

# Third-party imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Local imports
from src.utils.benchmark import BenchmarkSuite, PerformanceMetrics
from src.utils.numerical import clamp_probs

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)
logger = logging.getLogger(__name__)


def create_dummy_model(input_size: int = 3, img_size: int = 64) -> nn.Module:
    """Create a dummy model for demonstration."""
    return nn.Sequential(
        nn.Conv2d(input_size, 64, 3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.Linear(64, 1)
    )


def create_dummy_dataset(num_samples: int = 1000, img_size: int = 64) -> DataLoader:
    """Create a dummy dataset for demonstration."""
    X = torch.randn(num_samples, 3, img_size, img_size)
    y = torch.randint(0, 2, (num_samples,))
    
    dataset = TensorDataset(X, y)
    return DataLoader(
        dataset, batch_size=32, shuffle=True,
        num_workers=2, pin_memory=torch.cuda.is_available()
    )


def demo_mixed_precision_training():
    """Demonstrate mixed precision training benefits."""
    logger.info("üöÄ Demo: Mixed Precision Training")
    logger.info("=" * 50)
    
    device = get_device()
    logger.info(f"Using device: {device}")
    
    if device.type != 'cuda':
        logger.warning("Mixed precision requires CUDA. Skipping AMP demo.")
        return
    
    # Create model and data
    model = create_dummy_model()
    dataloader = create_dummy_dataset()
    
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)
    
    # Benchmark suite
    suite = BenchmarkSuite()
    
    # Test FP32 training
    logger.info("Testing FP32 training...")
    model_fp32 = create_dummy_model()
    model_fp32.to(device)
    
    metrics_fp32 = suite.benchmark_training(
        model_fp32, dataloader, criterion, optimizer, 
        num_epochs=1, use_amp=False, device=device
    )
    
    # Test AMP training
    logger.info("Testing AMP training...")
    model_amp = create_dummy_model()
    model_amp.to(device)
    
    metrics_amp = suite.benchmark_training(
        model_amp, dataloader, criterion, optimizer,
        num_epochs=1, use_amp=True, device=device
    )
    
    # Compare results
    speedup = metrics_amp.samples_per_second / metrics_fp32.samples_per_second
    memory_reduction = (metrics_fp32.gpu_memory_gb - metrics_amp.gpu_memory_gb) / metrics_fp32.gpu_memory_gb * 100
    
    logger.info(f"üìä AMP Results:")
    logger.info(f"  Speedup: {speedup:.2f}x")
    logger.info(f"  Memory reduction: {memory_reduction:.1f}%")
    logger.info(f"  FP32 throughput: {metrics_fp32.samples_per_second:.1f} samples/sec")
    logger.info(f"  AMP throughput: {metrics_amp.samples_per_second:.1f} samples/sec")


def demo_optimized_data_loading():
    """Demonstrate optimized data loading."""
    logger.info("\nüíæ Demo: Optimized Data Loading")
    logger.info("=" * 50)
    
    device = get_device()
    
    # Create datasets with different configurations
    configs = [
        {"num_workers": 0, "pin_memory": False, "name": "Basic"},
        {"num_workers": 2, "pin_memory": False, "name": "Multi-worker"},
        {"num_workers": 2, "pin_memory": True, "name": "Optimized"},
    ]
    
    model = create_dummy_model()
    model.to(device)
    model.eval()
    
    suite = BenchmarkSuite()
    
    for config in configs:
        logger.info(f"Testing {config['name']} configuration...")
        
        # Create dataloader with specific config
        X = torch.randn(1000, 3, 64, 64)
        y = torch.randint(0, 2, (1000,))
        dataset = TensorDataset(X, y)
        
        dataloader = DataLoader(
            dataset, batch_size=32, shuffle=False,
            num_workers=config['num_workers'],
            pin_memory=config['pin_memory']
        )
        
        # Benchmark inference
        metrics = suite.benchmark_inference(
            model, dataloader, use_amp=False, device=device
        )
        
        logger.info(f"  {config['name']}: {metrics.samples_per_second:.1f} samples/sec")


def demo_parallel_inference():
    """Demonstrate parallel model inference."""
    logger.info("\n‚ö° Demo: Parallel Model Inference")
    logger.info("=" * 50)
    
    device = get_device()
    
    # Create multiple models
    models = {
        "model_1": create_dummy_model(),
        "model_2": create_dummy_model(),
        "model_3": create_dummy_model(),
    }
    
    # Move models to device
    for model in models.values():
        model.to(device)
        model.eval()
    
    dataloader = create_dummy_dataset()
    suite = BenchmarkSuite()
    
    # Sequential inference
    logger.info("Testing sequential inference...")
    sequential_times = []
    
    for name, model in models.items():
        start_time = time.time()
        metrics = suite.benchmark_inference(model, dataloader, device=device)
        sequential_times.append(time.time() - start_time)
        logger.info(f"  {name}: {metrics.samples_per_second:.1f} samples/sec")
    
    total_sequential = sum(sequential_times)
    logger.info(f"Total sequential time: {total_sequential:.2f}s")
    
    # Parallel inference simulation (simplified)
    logger.info("Testing parallel inference (simulated)...")
    start_time = time.time()
    
    # Simulate parallel execution (in real implementation, this would use ThreadPoolExecutor)
    parallel_time = max(sequential_times)  # Best case: all models run simultaneously
    speedup = total_sequential / parallel_time
    
    logger.info(f"Parallel time (simulated): {parallel_time:.2f}s")
    logger.info(f"Parallel speedup: {speedup:.2f}x")


def demo_performance_monitoring():
    """Demonstrate performance monitoring capabilities."""
    logger.info("\nüìä Demo: Performance Monitoring")
    logger.info("=" * 50)
    
    device = get_device()
    
    # Create model and data
    model = create_dummy_model()
    dataloader = create_dummy_dataset()
    
    suite = BenchmarkSuite()
    
    # Run benchmark
    metrics = suite.benchmark_inference(model, dataloader, device=device)
    
    # Display detailed metrics
    logger.info("üìà Performance Metrics:")
    logger.info(f"  Throughput: {metrics.samples_per_second:.1f} samples/sec")
    logger.info(f"  Batch time: {metrics.avg_batch_time:.3f}s")
    logger.info(f"  Memory usage: {metrics.peak_memory_gb:.1f} GB")
    logger.info(f"  Model size: {metrics.model_size_mb:.1f} MB")
    logger.info(f"  Parameters: {metrics.num_parameters:,}")
    
    if metrics.gpu_memory_gb:
        logger.info(f"  GPU memory: {metrics.gpu_memory_gb:.1f} GB")
    
    if metrics.gpu_utilization:
        logger.info(f"  GPU utilization: {metrics.gpu_utilization:.1f}%")
    
    # Generate report
    report = suite.generate_report()
    logger.info("\nüìã Benchmark Report:")
    logger.info(report)


def demo_cloud_readiness():
    """Demonstrate cloud deployment readiness."""
    logger.info("\n‚òÅÔ∏è Demo: Cloud Deployment Readiness")
    logger.info("=" * 50)
    
    # Check system capabilities
    device = get_device()
    
    logger.info("üîç System Capabilities:")
    logger.info(f"  Device: {device}")
    logger.info(f"  CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        logger.info(f"  GPU count: {torch.cuda.device_count()}")
        logger.info(f"  GPU name: {torch.cuda.get_device_name(0)}")
        logger.info(f"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Check dependencies
    logger.info("\nüì¶ Dependencies:")
    try:
        import psutil
        logger.info(f"  psutil: {psutil.__version__}")
    except ImportError:
        logger.warning("  psutil: Not installed (required for memory monitoring)")
    
    try:
        import GPUtil
        logger.info(f"  GPUtil: Available")
    except ImportError:
        logger.warning("  GPUtil: Not installed (required for GPU monitoring)")
    
    # Performance recommendations
    logger.info("\nüí° Cloud Deployment Recommendations:")
    
    if device.type == 'cpu':
        logger.info("  - Use CPU-optimized instances for development")
        logger.info("  - Consider GPU instances for production training")
    else:
        logger.info("  - GPU instance ready for high-performance training")
        logger.info("  - Enable mixed precision for 2-3x speedup")
        logger.info("  - Use multi-GPU for large-scale training")
    
    logger.info("  - Enable optimized data loading (pin_memory=True)")
    logger.info("  - Use appropriate batch sizes for memory efficiency")
    logger.info("  - Monitor memory usage to prevent OOM errors")


def main():
    """Main demo function."""
    parser = argparse.ArgumentParser(description="P1 Performance & Scalability Demo")
    
    parser.add_argument("--quick", action="store_true",
                        help="Run quick demo (basic features only)")
    parser.add_argument("--full-demo", action="store_true",
                        help="Run full demo (all features)")
    parser.add_argument("--amp", action="store_true",
                        help="Include AMP demonstrations")
    parser.add_argument("--verbose", action="store_true",
                        help="Verbose logging")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info("üéØ P1 Performance & Scalability Demo")
    logger.info("=" * 60)
    
    # Run demos based on arguments
    if args.quick:
        logger.info("Running quick demo...")
        demo_optimized_data_loading()
        demo_performance_monitoring()
    elif args.full_demo:
        logger.info("Running full demo...")
        demo_optimized_data_loading()
        demo_performance_monitoring()
        demo_parallel_inference()
        demo_cloud_readiness()
        
        if args.amp and torch.cuda.is_available():
            demo_mixed_precision_training()
        elif args.amp:
            logger.warning("AMP demo skipped: CUDA not available")
    else:
        # Default: run all demos
        logger.info("Running default demo...")
        demo_optimized_data_loading()
        demo_performance_monitoring()
        demo_parallel_inference()
        demo_cloud_readiness()
        
        if torch.cuda.is_available():
            demo_mixed_precision_training()
    
    logger.info("\n‚úÖ P1 Demo completed!")
    logger.info("üöÄ Your system is ready for high-performance ML training!")


if __name__ == "__main__":
    main()

